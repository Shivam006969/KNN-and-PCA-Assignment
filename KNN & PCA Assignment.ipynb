{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee0456e-ef84-4d10-98c9-312ad4b78f7e",
   "metadata": {},
   "source": [
    "# KNN & PCA | **Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6e1e8-1d02-4f81-b5b0-ec97ff75f7d5",
   "metadata": {},
   "source": [
    "### 1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd64b2-88d3-4259-a823-81ede458f8ea",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors (KNN)** is a simple, non-parametric machine learning algorithm used for both **classification** and **regression**. It's considered a \"lazy\" learning algorithm because it doesn't build a model during the training phase. Instead, it memorizes the entire dataset and performs all the computational work during prediction.\n",
    "\n",
    "***\n",
    "\n",
    "#### How KNN Works\n",
    "\n",
    "The core idea of KNN is that an object's characteristics are similar to those of its neighbors. When you want to make a prediction for a new, unknown data point, the algorithm follows these steps:\n",
    "\n",
    "1.  **Choose a number K**: This is the number of neighbors to consider.\n",
    "2.  **Calculate Distance**: It calculates the distance from the new data point to all other points in the training dataset. Common distance metrics include Euclidean distance or Manhattan distance.\n",
    "3.  **Find the K-Nearest Neighbors**: The algorithm identifies the K data points in the training set that are closest to the new point based on the calculated distances.\n",
    "4.  **Make a Prediction**:\n",
    "    * **For Classification**: It looks at the classes of the K-nearest neighbors and predicts the class that is most common among them. For example, if K=5 and three neighbors are \"Class A\" and two are \"Class B\", the new data point will be classified as \"Class A\".\n",
    "    * **For Regression**: It takes the average of the values of the K-nearest neighbors to predict a continuous value for the new data point. For example, if K=5 and the neighbor values are [10, 12, 11, 15, 12], the predicted value would be the average, which is 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4d9b4-010a-4eba-aa97-7899e9c509aa",
   "metadata": {},
   "source": [
    "### 2. What is the Curse of Dimensionality and how does it affect KNN performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3270b15-7335-4452-a9db-4a8909282c91",
   "metadata": {},
   "source": [
    "The **Curse of Dimensionality** is a phenomenon that occurs when we add more features (or dimensions) to a dataset. As the number of dimensions increases, the data becomes extremely sparse, and the distance between any two points starts to become less meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "#### How it Affects KNN Performance\n",
    "\n",
    "The Curse of Dimensionality severely impacts KNN in two main ways:\n",
    "\n",
    "1.  **Diminished Relevance of Distance**: The core of KNN relies on finding the \"nearest\" neighbors. In a low-dimensional space, the distance between points is a good indicator of their similarity. However, in a high-dimensional space, all data points tend to be roughly equidistant from each other. The difference between the nearest neighbor and the farthest neighbor becomes negligible. This means that KNN's fundamental principle of using distance to determine similarity breaks down, and the algorithm essentially starts making random guesses.\n",
    "\n",
    "2.  **Increased Computational Cost**: As you add more dimensions, the time and memory required to compute the distance between a new point and all existing data points increases exponentially. This makes the KNN algorithm very slow and impractical for datasets with a large number of features.\n",
    "\n",
    "To combat this, it's crucial to perform **dimensionality reduction** before applying KNN to high-dimensional data. Techniques like **Principal Component Analysis (PCA)** can help by transforming the data into a lower-dimensional space while retaining as much of the original information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e27e4-04a7-4668-9f99-abe120840a23",
   "metadata": {},
   "source": [
    "### 3. What is Principal Component Analysis (PCA)? How is it different from feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604f5b7-a241-4cc1-bd7a-c34d37e775b6",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to simplify a dataset while retaining its most important information. It transforms a set of correlated variables into a smaller set of uncorrelated variables called **principal components**. The first principal component accounts for the largest possible variance in the data, the second component accounts for the next largest variance orthogonal to the first, and so on. Essentially, PCA creates new, synthesized features from the original ones. .\n",
    "\n",
    "***\n",
    "\n",
    "#### PCA vs. Feature Selection\n",
    "\n",
    "While both PCA and feature selection are techniques for dimensionality reduction, they achieve it in fundamentally different ways:\n",
    "\n",
    "* **PCA (Feature Extraction)**: PCA doesn't remove features; it transforms the existing ones into a new set of components. It creates a new, smaller set of features that are linear combinations of the original features. This can be very useful for reducing the complexity of a dataset and combating the \"curse of dimensionality.\" However, the new components can be difficult to interpret, as they don't directly correspond to any of the original variables.\n",
    "\n",
    "* **Feature Selection**: This is a process that chooses a **subset of the most relevant features** from the original dataset and discards the rest. The selected features are the actual, original variables. Feature selection methods can be simple (like removing features with low variance) or more complex (like using a model to rank feature importance). The primary advantage of feature selection is that the final model is more interpretable because it uses the original, understandable features. However, it may discard valuable information contained in the discarded features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a679ca-080d-4e8f-a732-a0b6e7cde8b7",
   "metadata": {},
   "source": [
    "### 4. What are eigenvalues and eigenvectors in PCA, and why are they important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be72777-4a85-4080-add1-5b9ed6c0d50f",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are fundamental concepts in linear algebra that are crucial for understanding how PCA works. They allow us to find the new coordinate system (the principal components) that best captures the variance in the data.\n",
    "\n",
    "#### Eigenvectors\n",
    "\n",
    "An **eigenvector** is a special vector that, when a linear transformation is applied, only changes in magnitude (is scaled) but not in direction. In PCA, the eigenvectors of the covariance matrix represent the **directions** or axes of the new feature space. These axes are the **principal components**. The first principal component corresponds to the eigenvector that points in the direction of the greatest variance in the data, the second to the next greatest, and so on.\n",
    "\n",
    "#### Eigenvalues\n",
    "\n",
    "An **eigenvalue** is the scalar value associated with an eigenvector. It represents the **magnitude** of the transformation in the direction of the eigenvector. In PCA, the eigenvalues of the covariance matrix tell us the **amount of variance** captured by each principal component. A larger eigenvalue means that its corresponding eigenvector (principal component) captures more variance. This is why we sort the eigenvalues in descending order and select the top few—they represent the most important components that contain the most information about the data.\n",
    "\n",
    "#### Why They're Important in PCA \n",
    "\n",
    "Eigenvalues and eigenvectors are so important because they provide a mathematical framework for performing the dimensionality reduction. We compute the covariance matrix of our data, find its eigenvectors and eigenvalues, and then use the eigenvectors with the largest eigenvalues to construct the new, lower-dimensional feature space. This ensures that we are retaining as much of the original data's variance as possible while reducing the number of dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd080955-8fe1-4cbe-8bba-5b69768dc654",
   "metadata": {},
   "source": [
    "### 5. How do KNN and PCA complement each other when applied in a single pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c254c-fb48-4738-8a7c-06d89dc5f0e6",
   "metadata": {},
   "source": [
    "KNN and PCA are a powerful combination in a data science pipeline because they address each other's weaknesses. PCA acts as a crucial preprocessing step for KNN, making the model more efficient and effective.\n",
    "\n",
    "***\n",
    "\n",
    "#### The Synergy of KNN and PCA:\n",
    "\n",
    "* **Solving the Curse of Dimensionality**: The primary benefit of using PCA before KNN is to mitigate the **curse of dimensionality**. As we discussed, KNN's performance degrades in high-dimensional spaces because the concept of \"nearest\" neighbors becomes less meaningful. By using PCA, you can reduce the number of features while preserving the most important variance in the data. This allows KNN to operate in a more manageable, lower-dimensional space where distances are more reliable, leading to better predictive accuracy.\n",
    "\n",
    "* **Improving Computational Efficiency**: KNN is a **lazy algorithm** that requires calculating the distance to every data point during prediction, which can be very slow with large numbers of features.  By using PCA to reduce the feature count, we dramatically decrease the number of calculations needed for each prediction. This makes the entire pipeline much faster and more scalable, allowing us to use KNN on datasets that would otherwise be too large or complex.\n",
    "\n",
    "In essence, PCA provides a more compact and meaningful representation of the data, which directly benefits KNN's performance and efficiency. The pipeline becomes:\n",
    "\n",
    "**`Data --> PCA (Dimensionality Reduction) --> KNN (Classification/Regression)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8afc48-d03c-478d-b4a1-f035eef9eb6c",
   "metadata": {},
   "source": [
    "### 6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d5fb29-9720-4bfd-96ea-e6dbd25f4ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without feature scaling:\n",
      "0.7037\n",
      "\n",
      "Accuracy with feature scaling:\n",
      "0.9815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "# The Wine dataset is a classic classification problem with 13 features.\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# 2. Case 1: Train KNN without feature scaling\n",
    "# We train the KNN classifier directly on the raw data.\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "print(\"Accuracy without feature scaling:\")\n",
    "print(f\"{accuracy_unscaled:.4f}\")\n",
    "\n",
    "# 3. Case 2: Train KNN with feature scaling\n",
    "# We use StandardScaler to normalize the features. This is a crucial step\n",
    "# for distance-based algorithms like KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the KNN classifier on the scaled data.\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(\"\\nAccuracy with feature scaling:\")\n",
    "print(f\"{accuracy_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562e9ab-f3ef-4daa-9f5d-7f9880e98e25",
   "metadata": {},
   "source": [
    "### 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4cfbe0-6c5c-49bf-9a76-be7b6bdf925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without feature scaling:\n",
      "0.7407\n",
      "\n",
      "Accuracy with feature scaling:\n",
      "0.9630\n",
      "\n",
      "Explained Variance Ratio of Principal Components:\n",
      "[0.36196226 0.18763862 0.11656548 0.07578973 0.07043753 0.04552517\n",
      " 0.03584257 0.02646315 0.02174942 0.01958347 0.01762321 0.01323825\n",
      " 0.00758114]\n",
      "\n",
      "Cumulative explained variance of the first two components: 0.5496\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "# The Wine dataset is a classic classification problem with 13 features.\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Case 1: Train KNN without feature scaling\n",
    "# We train the KNN classifier directly on the raw data.\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "print(\"Accuracy without feature scaling:\")\n",
    "print(f\"{accuracy_unscaled:.4f}\")\n",
    "\n",
    "# 3. Case 2: Train KNN with feature scaling\n",
    "# We use StandardScaler to normalize the features. This is a crucial step\n",
    "# for distance-based algorithms like KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the KNN classifier on the scaled data.\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(\"\\nAccuracy with feature scaling:\")\n",
    "print(f\"{accuracy_scaled:.4f}\")\n",
    "\n",
    "# 4. Train a PCA model and print explained variance ratio\n",
    "# We train a PCA model on the scaled training data.\n",
    "# The `explained_variance_ratio_` attribute shows the proportion of\n",
    "# variance in the data that is captured by each principal component.\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance Ratio of Principal Components:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "# Sum of the first two components to show cumulative variance\n",
    "cumulative_variance = np.sum(explained_variance_ratio[:2])\n",
    "print(f\"\\nCumulative explained variance of the first two components: {cumulative_variance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d7935-b52c-41f8-b0d5-f6c5c47e220f",
   "metadata": {},
   "source": [
    "### 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0d8412-383a-45b8-ad7d-42630b782a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without feature scaling:\n",
      "0.7407\n",
      "\n",
      "Accuracy with feature scaling:\n",
      "0.9630\n",
      "\n",
      "Explained Variance Ratio of Principal Components:\n",
      "[0.36196226 0.18763862 0.11656548 0.07578973 0.07043753 0.04552517\n",
      " 0.03584257 0.02646315 0.02174942 0.01958347 0.01762321 0.01323825\n",
      " 0.00758114]\n",
      "\n",
      "Cumulative explained variance of the first two components: 0.5496\n",
      "\n",
      "Accuracy with PCA (top 2 components):\n",
      "0.9815\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "# The Wine dataset is a classic classification problem with 13 features.\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Case 1: Train KNN without feature scaling\n",
    "# We train the KNN classifier directly on the raw data.\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "print(\"Accuracy without feature scaling:\")\n",
    "print(f\"{accuracy_unscaled:.4f}\")\n",
    "\n",
    "# 3. Case 2: Train KNN with feature scaling\n",
    "# We use StandardScaler to normalize the features. This is a crucial step\n",
    "# for distance-based algorithms like KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the KNN classifier on the scaled data.\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(\"\\nAccuracy with feature scaling:\")\n",
    "print(f\"{accuracy_scaled:.4f}\")\n",
    "\n",
    "# 4. Train a PCA model and print explained variance ratio\n",
    "# We train a PCA model on the scaled training data.\n",
    "# The `explained_variance_ratio_` attribute shows the proportion of\n",
    "# variance in the data that is captured by each principal component.\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance Ratio of Principal Components:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "# Sum of the first two components to show cumulative variance\n",
    "cumulative_variance = np.sum(explained_variance_ratio[:2])\n",
    "print(f\"\\nCumulative explained variance of the first two components: {cumulative_variance:.4f}\")\n",
    "\n",
    "# 5. Case 3: Train KNN on PCA-transformed data (top 2 components)\n",
    "# We use PCA to reduce the dimensionality to the top 2 principal components.\n",
    "# This helps to remove noise and can improve performance for KNN.\n",
    "pca_2_components = PCA(n_components=2)\n",
    "X_train_pca = pca_2_components.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
    "\n",
    "# Train a new KNN classifier on the PCA-transformed data\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(\"\\nAccuracy with PCA (top 2 components):\")\n",
    "print(f\"{accuracy_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7eebaf-4a7d-42b5-9400-cd9c0be8cf40",
   "metadata": {},
   "source": [
    "### 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d16b0e1-d43d-419a-8fe7-7bd94221ea6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without feature scaling:\n",
      "0.7407\n",
      "\n",
      "Accuracy with feature scaling:\n",
      "0.9630\n",
      "\n",
      "Explained Variance Ratio of Principal Components:\n",
      "[0.36196226 0.18763862 0.11656548 0.07578973 0.07043753 0.04552517\n",
      " 0.03584257 0.02646315 0.02174942 0.01958347 0.01762321 0.01323825\n",
      " 0.00758114]\n",
      "\n",
      "Cumulative explained variance of the first two components: 0.5496\n",
      "\n",
      "Accuracy with PCA (top 2 components):\n",
      "0.9815\n",
      "\n",
      "Accuracy with Manhattan distance on scaled data:\n",
      "0.9630\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "# The Wine dataset is a classic classification problem with 13 features.\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Case 1: Train KNN without feature scaling\n",
    "# We train the KNN classifier directly on the raw data.\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "print(\"Accuracy without feature scaling:\")\n",
    "print(f\"{accuracy_unscaled:.4f}\")\n",
    "\n",
    "# 3. Case 2: Train KNN with feature scaling\n",
    "# We use StandardScaler to normalize the features. This is a crucial step\n",
    "# for distance-based algorithms like KNN.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the KNN classifier on the scaled data.\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(\"\\nAccuracy with feature scaling:\")\n",
    "print(f\"{accuracy_scaled:.4f}\")\n",
    "\n",
    "# 4. Train a PCA model and print explained variance ratio\n",
    "# We train a PCA model on the scaled training data.\n",
    "# The `explained_variance_ratio_` attribute shows the proportion of\n",
    "# variance in the data that is captured by each principal component.\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"\\nExplained Variance Ratio of Principal Components:\")\n",
    "print(explained_variance_ratio)\n",
    "\n",
    "# Sum of the first two components to show cumulative variance\n",
    "cumulative_variance = np.sum(explained_variance_ratio[:2])\n",
    "print(f\"\\nCumulative explained variance of the first two components: {cumulative_variance:.4f}\")\n",
    "\n",
    "# 5. Case 3: Train KNN on PCA-transformed data (top 2 components)\n",
    "# We use PCA to reduce the dimensionality to the top 2 principal components.\n",
    "# This helps to remove noise and can improve performance for KNN.\n",
    "pca_2_components = PCA(n_components=2)\n",
    "X_train_pca = pca_2_components.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
    "\n",
    "# Train a new KNN classifier on the PCA-transformed data\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(\"\\nAccuracy with PCA (top 2 components):\")\n",
    "print(f\"{accuracy_pca:.4f}\")\n",
    "\n",
    "# 6. Case 4: KNN with different distance metrics on scaled data\n",
    "# Euclidean distance (p=2) is the default, so we'll also test Manhattan (p=1).\n",
    "# We use the scaled data, as it is the most robust for distance-based models.\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, p=1)\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "print(\"\\nAccuracy with Manhattan distance on scaled data:\")\n",
    "print(f\"{accuracy_manhattan:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58cfd69-1c12-43fb-9644-f3c7cefadd48",
   "metadata": {},
   "source": [
    "### 10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
    "Due to the large number of features and a small number of samples, traditional models overfit.\n",
    "Explain how you would:\n",
    "- Use PCA to reduce dimensionality\n",
    "- Decide how many components to keep\n",
    "- Use KNN for classification post-dimensionality reduction\n",
    "- Evaluate the model\n",
    "- Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60e519-3dc8-41d0-913c-0dc9041d3777",
   "metadata": {},
   "source": [
    "### Pipeline for High-Dimensional Gene Expression Classification\n",
    "\n",
    "#### 1. **Use PCA to Reduce Dimensionality**\n",
    "\n",
    "Gene expression data often has thousands of features (genes) and very few samples. PCA helps by transforming the data into a set of orthogonal components that capture the most variance.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Decide How Many Components to Keep**\n",
    "\n",
    "We would want to retain enough components to preserve most of the variance while reducing noise and overfitting risk.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Choose number of components that explain 95% variance\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "\n",
    "# Re-apply PCA with selected components\n",
    "pca = PCA(n_components=n_components)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "```\n",
    "---\n",
    "\n",
    "#### 3. **Use KNN for Classification Post-PCA**\n",
    "\n",
    "KNN is simple, non-parametric, and works well when the feature space is clean and reduced.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tune the number of neighbors\n",
    "param_grid = {'n_neighbors': list(range(1, 21))}\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Evaluate the Model**\n",
    "\n",
    "Use stratified cross-validation and multiple metrics to ensure robustness.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "---\n",
    "\n",
    "#### 5. **Justify the Pipeline to Stakeholders**\n",
    "\n",
    "Here’s how it has happened:\n",
    "\n",
    "- **Dimensionality Reduction:** PCA combats overfitting and reveals latent biological signals.\n",
    "- **Interpretability:** PCA components can be traced back to gene contributions, aiding biological insight.\n",
    "- **Simplicity & Transparency:** KNN is intuitive and easy to explain, especially in clinical settings.\n",
    "- **Validation:** Cross-validation and metric-based evaluation ensure the model generalizes well.\n",
    "- **Scalability:** The pipeline is modular and can be extended to other classifiers or integrated with biological priors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
